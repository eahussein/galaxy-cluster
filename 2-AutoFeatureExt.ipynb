{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd3a024-5f12-47ce-8511-c2bfae262628",
   "metadata": {},
   "source": [
    "# Tutorial 2: Automatic Feature Extraction/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed665b16-22c8-4a4d-b4db-0837264fb837",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a99b7c-2540-4862-bcca-315eeb1c0803",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5412c-eb3a-416a-81ff-ce5f77a0df88",
   "metadata": {},
   "source": [
    "In this notebook, we will extract/engineer features using a deep learning method called autoencoder.\n",
    " An autoencoder is an artificial neural network with a symmetric structure which is trained to reconstruct its input at the final output layer. The output of the first half of the network represents an encoding of the input data. ([source](https://arxiv.org/abs/2206.06165))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030153b-d68e-4f25-8774-4b9e4bb5bfef",
   "metadata": {},
   "source": [
    "First, we import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d5526-086d-4d0e-b546-e81af265749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting data/graphs\n",
    "import numpy as np # For handling N-DIMENSIONAL ARRAYS\n",
    "\n",
    "import tensorflow as tf #An end-to-end machine learning platform, focusing on training deep learning models\n",
    "from tensorflow.keras import layers, losses # Implementation of the Keras API, the high-level API of TensorFlow.\n",
    "from tensorflow.keras.models import Model #This displays graphs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7b065-2cd2-4d88-8329-a7b0e9591e1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a52a2-6542-4535-9aad-0782ed0eb4a4",
   "metadata": {},
   "source": [
    "### Reading in data\n",
    "The following code is the same as in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ba1a6-c0cd-4bb9-ae83-74143f138397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxy_mnist import GalaxyMNISTHighrez\n",
    "\n",
    "dataset_train = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=True  # by default, or False for canonical test set\n",
    ")\n",
    "# for the testing data\n",
    "dataset_test = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=False  # by default, or False for canonical test set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4639a4d-6649-48e5-8fec-47d8026828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the training and testing labels and image samples\n",
    "images_train = dataset_train.data\n",
    "images_test = dataset_test.data\n",
    "labels_train = dataset_train.targets\n",
    "labels_test = dataset_test.targets\n",
    "classes = GalaxyMNISTHighrez.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ad8d2-8d8c-4c37-8b9d-13d4d9a48a93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4d509-8977-4f8d-a158-8a0bfc75db0b",
   "metadata": {},
   "source": [
    "### Pre-processing \n",
    "The following code is the same as in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6decf49-9a17-43b2-b659-6a429440326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.pre import pre_processing #  A predefined function to pre-process the data as we did in tutorial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9da92-41d1-41b7-a888-593598261aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing(data, size) function takes two arguments\n",
    "# 1. data: the data to be processed\n",
    "# 2. The size for which the data needs to be reduced.\n",
    "images_trainPre = pre_processing(images_train, 56)\n",
    "images_testPre = pre_processing(images_test, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d6bae-5885-4ce3-83d7-74dbdaeb8b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 56, 56)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_trainPre.shape # the shape of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba698cf5-b955-43a1-8dcd-e97f4088e2de",
   "metadata": {},
   "source": [
    "Displaying images after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb62536-7817-4f05-9110-263a0f57da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1\n",
    "columns = 5\n",
    "for j in range(len(GalaxyMNISTHighrez.classes)):\n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (columns):    # Create images in each column\n",
    "        train_image = images_trainPre[(labels_train == j)][i]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(train_image*255,cmap='gray', vmin=0, vmax=255) \n",
    "                            # we have to multiply the image by 255 to restore the original values\n",
    "    print(\"label: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444a5c0-dd53-49df-bcd0-fa56e2f874d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd1b55-87ec-4794-925a-c9bcf919c35f",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "We will try different options for the autoencoder and compare performance. First we'll try a shallow autoencoder with as few layers as possible. Then we'll compare with a deeper autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc59a2f-14d3-47db-90d0-65454e41db44",
   "metadata": {},
   "source": [
    "#### Shallow Autoencoder\n",
    "The autoencoder neural network must have a symmetric structure, and thus must have an even number of layers. We will use a very simple neural network for the autoencoder with just two hidden layers (plus input and output layers). The autoencoder neural network is trained on the data that we pre-processed. The original code can be found [here](https://www.tensorflow.org/tutorials/generative/autoencoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91913044-6f3c-475f-8fc8-c6ce791b5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128 # the number of features to be encoded, this can change \n",
    "num, length, width  = images_trainPre.shape\n",
    "# need to document how excatly it works\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    # The NN is defined in two parts:encoder and decoder\n",
    "    # Encoder part:\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(), # Input layer-- flattens image into vector\n",
    "      layers.Dense(latent_dim, activation='relu'), # Dense hidden layer\n",
    "    ])\n",
    "    # Decoder part of the NN\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(length*width, activation='sigmoid'), # Dense hidden layer\n",
    "      layers.Reshape((length, width)) # Output layer (reshapes vector back to image size)\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c9344-d9aa-410c-9ffc-aba368e6f963",
   "metadata": {},
   "source": [
    "##### 1) Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ad6c0-dea1-41b3-b06b-d97c495dbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a8a4d-00b1-4c71-ba94-19e2e3696994",
   "metadata": {},
   "source": [
    "##### 2) Compile model with Adam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a790d-e822-4931-aa81-a08dd2139d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360fe70-e8b0-4124-95a6-f82ecf1a65ad",
   "metadata": {},
   "source": [
    "##### 3) Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa553281-1bcc-4644-9431-337f7c6e9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.build((None, 56,56,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e35a5-3463-43ec-9104-9146b5a087cb",
   "metadata": {},
   "source": [
    "##### 4) Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afdbd5",
   "metadata": {},
   "source": [
    "In the training process we use \"early stopping\", which automatically terminates training when there is little or no improvement from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74973b4d-eed4-44c5-a4b9-56f64a8098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14142d42-8a62-4b58-9da4-4ae5cf6eb163",
   "metadata": {},
   "source": [
    "`EarlyStopping()` has a few options:\n",
    "- `monitor (default value 'val_loss')`: Uses validation loss as performance measure to terminate the training.\n",
    "- `patience (default value 0)`: specifies the number of epochs with no improvement. The value 0 means the training is terminated as soon as the performance measure gets worse from one epoch to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfef81",
   "metadata": {},
   "source": [
    "We're ready to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6b051-ff28-40d8-89da-1707effe9a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shallow_model.fit(np.array(images_trainPre), np.array(images_trainPre),\n",
    "                epochs=50,\n",
    "                shuffle=True,\n",
    "                validation_data=(np.array(images_trainPre), np.array(images_trainPre)), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8d8c7-be81-4aec-a199-f581a8086d59",
   "metadata": {},
   "source": [
    "This code runs very fast, because the model is very shallow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4e5cc-cac4-47bc-8973-d1c37c4c478d",
   "metadata": {},
   "source": [
    "##### 5) Display the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5329236",
   "metadata": {},
   "source": [
    "Now, Let's compare inputs and outputs, and see if they closely resemble each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ae8e9-3ec3-42e1-bcd1-ccc403c6e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = shallow_model.encoder(images_testPre).numpy()\n",
    "decoded_imgs = shallow_model.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae370e2-1298-44f9-9116-4ff3fb40bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the output shape is correct\n",
    "print(decoded_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfc53b-ee82-4e86-95b8-1db0953c618d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display inputs and outputs\n",
    "rows = 1\n",
    "columns = 5\n",
    "for j in range(len(GalaxyMNISTHighrez.classes)):\n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (columns):    # Create images in each column\n",
    "        test_image = images_testPre[(labels_test == j)][i]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(test_image*255,cmap='gray', vmin=0, vmax=255) \n",
    "                            # we have to multiply the image by 255 to restore the original values\n",
    "    print(\"Original: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (columns):    # Create images in each column\n",
    "        test_image = decoded_imgs[(labels_test == j)][i]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(test_image*255,cmap='gray', vmin=0, vmax=255) \n",
    "                            # we have to multiply the image by 255 to restore the original values\n",
    "    print(\"Reconstructed: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc42e49-a2a5-4b60-bb7b-b6b24c716ee8",
   "metadata": {},
   "source": [
    "**Exercise 1:** Which classes do you think will be confused with the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ebe2f-aa22-4283-84d4-1bf2adb00237",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Answer here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf77fc-e573-4e2f-a280-f58a579b41dd",
   "metadata": {},
   "source": [
    "##### 6) Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1a071-4433-4ad4-b4d1-e8f145ed5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.save(\"./shallowModel_save\") # saving the model (shallow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d10703-99a6-4cda-956a-90593104d410",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed9483-f19e-468c-a27b-59024b7ca62d",
   "metadata": {},
   "source": [
    "#### Deep convolutional autoencoder\n",
    "A visual comparison of inputs and outputs shows the shallow fully-connected autoencoder does not preserve images very well. So we'll try a more complicated model and compare execution time and image quality. In image classification, convolutional NN's are typically used. So let's try a convolutional NN for our autoencoder. The following autoencoder desing is from @@@ Give reference to original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea26ce-78ce-4870-ab1b-968ac82953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyEncoder(Model):\n",
    "    def __init__(self):\n",
    "        super(GalaxyEncoder, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential ([\n",
    "            layers.InputLayer(input_shape=(56,56,1)),\n",
    "            layers.Conv2D(16, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.MaxPool2D((2,2), padding=\"same\", strides=2),\n",
    "            layers.Conv2D(8, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.MaxPool2D((2,2), padding=\"same\", strides=2),\n",
    "            layers.Flatten()\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential ([\n",
    "            layers.InputLayer(input_shape=(1568)),\n",
    "            layers.Reshape((14, 14, 8)),\n",
    "            layers.UpSampling2D((2,2)),\n",
    "            layers.Conv2DTranspose(8, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.UpSampling2D((2,2)),\n",
    "            layers.Conv2DTranspose(16, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2D(1, (3,3), 1, padding=\"same\", activation=\"sigmoid\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c52cf9-7aa9-4252-8610-09682743fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = GalaxyEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a11030",
   "metadata": {},
   "source": [
    "### Exercises ###\n",
    "Following the procedure that was used above for the shallow model, apply the same steps to the deep model that we have just defined. These steps include: 1) define, 2) compiling, 3) building, 4) training, 5) displaying and 6) saving.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e24aef",
   "metadata": {},
   "source": [
    "##### 1) Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4466a-750f-449a-8efa-7a5bb77ee521",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b7d29-edec-490f-a1da-9e5cc5a7438e",
   "metadata": {},
   "source": [
    "##### 2) compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cc1f6-15bc-4494-b78d-228df4abef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911a00c-b5e6-48e6-a757-3b898e89dc55",
   "metadata": {},
   "source": [
    "##### 3) building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358be37-4e83-4adc-a503-1aa4219d7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ffb73-cf0f-46af-82a8-634449909747",
   "metadata": {},
   "source": [
    "##### 4) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b341f1f-cf28-4483-9643-e50ec8cdb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f863b13-dda8-4a09-a4a1-65c397773db5",
   "metadata": {},
   "source": [
    "##### 5) displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131900f-487d-4bb7-bbdb-0fc9bba67c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e2eec-e039-42eb-9ed0-cd1a5c59f029",
   "metadata": {},
   "source": [
    "##### 6) saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d615f-f5ff-4f91-9ea9-77eb13667a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861c799-f263-46b9-b8c4-f0767b65febc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb2e54-2920-417a-89ec-127c94212130",
   "metadata": {},
   "source": [
    "### Extracting the engineered features from the autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a50ad-a58a-421f-8be9-d45dbea2e8b6",
   "metadata": {},
   "source": [
    "The convolutional NN did no better than the shallow autoencoder.  So we will continue just with the shallow model.  To proceed, we need to extract the 64 encoded features from the shallow encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3843e6-6456-48f1-8335-986f991d648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96f82d-77de-49de-9be0-5164aba72e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('shallowModel_save/') # recalling the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5e701",
   "metadata": {},
   "source": [
    "We may apply the encoder to training and testing data to obtained the encoded features for data item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3feec-ac0d-4f5d-b2d8-15a67cfbe68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_features_train = model.encoder.predict(images_trainPre) # extracting the features for the training data\n",
    "auto_features_test = model.encoder.predict(images_testPre)   # extracting the features for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec886b-9488-490a-b6f3-6617a4d4737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_train = pd.DataFrame(auto_features_train) #turning the data into a dataframe\n",
    "auto_df_test = pd.DataFrame(auto_features_test) #turning the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18753f0e-2865-4fa4-97f0-de025fcba2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auto_df_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33736501-e002-46a3-ac7c-4c0db4c38b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42698a-ea07-4167-8939-d2f3f711f05f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca031aad-2a6d-42a2-bc71-ce97eabd391c",
   "metadata": {},
   "source": [
    "#### **_Saving data for later use_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaddef7-3663-41bb-8f6c-b6d1d26cae85",
   "metadata": {},
   "source": [
    "We can save the data so that we can call it up again in subsequent notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80fb440-3e58-4562-adc3-41e001c18acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store auto_df_train\n",
    "%store auto_df_test\n",
    "%store labels_train\n",
    "%store labels_test\n",
    "%store classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04993e78-0899-48f9-b9b3-abf6603ce9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36b6d3-5e3b-4215-b196-5827c7618b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gal_ker",
   "language": "python",
   "name": "gal_ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
